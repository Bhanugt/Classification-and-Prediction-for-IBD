# -*- coding: utf-8 -*-
"""LogisticModel

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AqATGtXl0XxZqUF5jLNX9c_wQwM685RR
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import joblib

from sklearn.preprocessing import LabelEncoder

# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns

# Apply Label Encoding for categorical columns
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le  # Store encoders for inverse transformation if needed

print("âœ… Categorical Data Converted to Numeric Format!")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load dataset
df = pd.read_csv("/content/sample_data/synthetic_ibd_dataset.csv")

# Convert categorical values to numeric using Label Encoding
categorical_cols = df.select_dtypes(include=['object']).columns
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Define features (X) and target (y)
X = df.drop(columns=["IBD_Type"])  # Modify "IBD_Type" to match your dataset
y = df["IBD_Type"]

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert target variable to integer labels (if needed)
y = y.astype(int)

# Split dataset (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print("âœ… Data Loaded & Preprocessed Successfully!")

# Define hyperparameter grid
param_grid = {
    "C": [0.01, 0.1, 1, 10, 100],
    "solver": ["liblinear"],  # Supports L1 and L2 penalty
    "penalty": ["l1", "l2"]
}

# Apply GridSearchCV for best parameters
grid_search = GridSearchCV(LogisticRegression(max_iter=3000), param_grid, cv=10, scoring="accuracy", n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# Get the best model
best_lr = grid_search.best_estimator_

print("ðŸ”¥ Best Hyperparameters:", grid_search.best_params_)

# Make predictions on test data
y_pred = best_lr.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("âœ… Logistic Regression Accuracy:", round(accuracy * 100, 2), "%")

# Print Confusion Matrix
print("ðŸ“Š Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Print Classification Report
print("ðŸ“„ Classification Report:\n", classification_report(y_test, y_pred))

# Save the trained Logistic Regression model
joblib.dump(best_lr, "logistic_regression_model.pkl")

print("âœ… Model saved as 'logistic_regression_model.pkl'!")